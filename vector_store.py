import os
import re
import chromadb
from embedder import to_embedding
from db import insert_paragraph
from llm import extract_keywords
from dotenv import load_dotenv
from sklearn.feature_extraction.text import TfidfVectorizer

load_dotenv()

chroma_host = os.getenv("CHROMA_HOST", "localhost")
chroma_port = os.getenv("CHROMA_PORT", "8000")
collection_name = os.getenv("CHROMA_COLLECTION", "rag_demo")

client = chromadb.HttpClient(host=chroma_host, port=int(chroma_port))

try:
    collection = client.get_collection(collection_name)
except:
    collection = client.create_collection(collection_name)


def add_to_vector_store(paragraphs, file_id, filename):
    print(f"‚úÖ Êñ∞Â¢ûÊÆµËêΩÂêëÈáèÔºö{len(paragraphs)} Á≠Ü\n")

    for idx, text in enumerate(paragraphs):
        chroma_id = f"{file_id}-{idx}"
        embedding = to_embedding(text)

        # Step 1: GPT ÊäΩÂá∫ÂéüÂßãÈóúÈçµË©û
        raw_keywords = extract_keywords(text)
        cleaned = clean_keywords(raw_keywords)

        # print("‚ö† GPT ÂéüÂßãÈóúÈçµÂ≠óÔºö", raw_keywords)
        # Step 2: ‰ΩøÁî® TF-IDF Â∞ç cleaned ÈóúÈçµË©ûÂÅöÊéíÂ∫èÈÅéÊøæ
        # final_keywords = extract_keywords_tfidf(text, cleaned)
        final_keywords = "„ÄÅ".join(cleaned)

        # Á¢∫‰øù metadata ‰∏≠ÁöÑ paragraph_id ÊòØÂ≠ó‰∏≤È°ûÂûã
        collection.add(
            ids=[chroma_id],
            embeddings=[embedding],
            documents=[text],
            metadatas={
                "file_id": str(file_id),
                "filename": filename,
                "paragraph_id": str(idx)  # Á¢∫‰øùÊòØÂ≠ó‰∏≤Ê†ºÂºè
            }
        )

        # Â∞á chroma_id ÂÑ≤Â≠òÂà∞Ë≥áÊñôÂ∫´‰∏≠ÔºåÁ¢∫‰øùËàá Chroma ‰∏≠ÁöÑ ID ‰∏ÄËá¥
        insert_paragraph(file_id, idx, text, chroma_id, final_keywords)


# Âú® vector_store.py ‰∏≠‰øÆÊîπÁõ∏‰ººÂ∫¶Ë®àÁÆó
def query_similar(query_text, top_k=5):
    query_embedding = to_embedding(query_text)
    results = collection.query(query_embeddings=[query_embedding], n_results=top_k,
                               include=["metadatas", "distances", "documents"])

    similar_results = []

    # Ê™¢Êü•ÂõûÂÇ≥ÁöÑÁµêÊûúÊòØÂê¶ÁÇ∫Á©∫
    if not results["ids"] or len(results["ids"][0]) == 0:
        print("‚ö†Ô∏è Êü•Ë©¢ÁµêÊûúÁÇ∫Á©∫")
        return similar_results

    for i, meta in enumerate(results["metadatas"][0]):
        chroma_id = results["ids"][0][i]
        file_id = meta.get("file_id", "")
        paragraph_id = meta.get("paragraph_id", "")

        # Á∞°ÂåñÁõ∏‰ººÂ∫¶Ë®àÁÆóÔºåÁõ¥Êé•‰ΩøÁî® Chroma ËøîÂõûÁöÑÈ§òÂº¶Áõ∏‰ººÂ∫¶
        distance = results["distances"][0][i]
        # Áõ∏‰ººÂ∫¶Ë®àÁÆóÔºöÈ§òÂº¶Ë∑ùÈõ¢ËΩâÁÇ∫Áõ∏‰ººÂ∫¶ÁôæÂàÜÊØî
        similarity = min(max((1 - distance) * 100, 0), 100)  # Á¢∫‰øùÂú® 0-100% ‰πãÈñì

        document = results["documents"][0][i]
        print(f"üî¢ ÂéüÂßãË∑ùÈõ¢ÂÄº: {distance}, Ë®àÁÆóÂæåÁõ∏‰ººÂ∫¶: {similarity}%")
        similar_results.append({
            "chroma_id": chroma_id,
            "score": similarity,  # Áõ∏‰ººÂ∫¶ÁôæÂàÜÊØî
            "paragraph_id": paragraph_id,
            "file_id": file_id,
            "text": document
        })

    return similar_results


def clean_keywords(raw_keywords: str) -> list:
    return [
        re.sub(r"[^\w\d‰∏Ä-Èæ•]", "", kw).strip()
        for kw in raw_keywords.split(",")
        if 1 < len(kw.strip()) <= 8
    ]


def extract_keywords_tfidf(text, candidate_keywords: list, top_k=10) -> str:
    if not candidate_keywords:
        return ""

    vectorizer = TfidfVectorizer(vocabulary=candidate_keywords, token_pattern=r"(?u)\b\w+\b")
    tfidf_matrix = vectorizer.fit_transform([text])
    scores = zip(vectorizer.get_feature_names_out(), tfidf_matrix.toarray()[0])
    sorted_keywords = sorted(scores, key=lambda x: x[1], reverse=True)

    return "„ÄÅ".join([word for word, _ in sorted_keywords[:top_k]])